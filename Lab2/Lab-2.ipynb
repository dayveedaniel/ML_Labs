{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "encouraging-navigator",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Week 2 : Linear Regression\n",
    "```\n",
    "- Machine Learning, Innopolis University (Fall semester 2024)\n",
    "- Instructors: Adil Khan & Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Linear Regression\n",
    "2. Multiple Linear Regression\n",
    "3. Polynomial Regression\n",
    "3.1 Overfitting & underfitting\n",
    "```\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-tomato",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"https://www.alpharithms.com/wp-content/uploads/1156/slope-intercept-vs-linear-regression-equations.jpg\"/>\n",
    "\n",
    "\n",
    "\n",
    "## Typical ML model Training Procedure\n",
    "\n",
    "1. Define the Task (Supervised or unsupervised Learning, Classification or Regression)  \n",
    "1. Import necessary libraries\n",
    "1. Load and Explore the dataset (visualization) \n",
    "1. Preprocess the Data & split to train/test  (feature selection, scaling, normalization, etc..)\n",
    "1. Build, Train and Test Model\n",
    "1. Evaluate model using appropiate evaluation metrics (MSE, $R^2$, accuracy, f-score, precision, recall, etc.. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-victim",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Linear Regression\n",
    "\n",
    "In this regression task we will predict the percentage of marks that a student is expected to score based upon the number of hours they studied.\n",
    "<br>\n",
    "<br>\n",
    "<center><b>Simple Linear Regression</b></center>\n",
    "$$y = \\beta_0 + \\beta_1x_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-conclusion",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4592863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-afghanistan",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-participant",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Load from database and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-hormone",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "session_engine = create_engine('sqlite:///ml-regression.db')\n",
    "\n",
    "with session_engine.connect() as conn: \n",
    "    student_scores_df = pd.read_sql_query(text('SELECT * FROM student_scores'), conn) \n",
    "\n",
    "student_scores_df.drop(columns=['index'], inplace=True)\n",
    "student_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-globe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3.1 Data profile and Statistics \n",
    "\n",
    "<span style=\"color:red\">Task : Get data profile and save it as `.html`</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-transcript",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# TODO: Task\n",
    "student_scores_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-favor",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3.1 Visualization\n",
    "\n",
    "Plot the data points on 2-D graph to eyeball our dataset and see if we can manually find any relationship between the data points. Usually Matplotlib is used but some other libraries exist i.e [Plotly](https://github.com/plotly/plotly.py), [Seaborn](https://seaborn.pydata.org/),[Geoplotlib](https://github.com/andrea-cuttone/geoplotlib/wiki/User-Guide), [Gleam](https://github.com/dgrtwo/gleam), [ggplot](https://github.com/tidyverse/ggplot2)\n",
    "\n",
    "**Remember :** Plot should have a title, axis labels, and legend for easy interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-belfast",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "student_scores_df.plot(x='Hours', y='Scores', style='o')\n",
    "plt.title('Hours vs Percentage')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Percentage Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-island",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Preprocess the Data & split to train set and test set\n",
    "\n",
    "<span style=\"color:red\">Task : split the data into train (80%) and test (20%)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-bronze",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = student_scores_df.iloc[:, :-1].values\n",
    "y = student_scores_df.iloc[:, 1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-territory",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Build, Train and Test ModelLinear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-likelihood",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print(f\"Model intercept : {regressor.intercept_}\")\n",
    "print(f\"Model coefficient : {regressor.coef_}\")\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "eval_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-underwear",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-comparison",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.5 Evaluate model using appropiate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-florida",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-bhutan",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Multiple Linear Regression\n",
    "\n",
    "Linear regression involving multiple variables is called \"multiple linear regression\"\n",
    "We will use multiple linear regression to predict the gas consumptions (in millions of gallons) in 48 US states based upon gas taxes (in cents), per capita income (dollars), paved highways (in miles) and the proportion of population that has a drivers license.\n",
    "We seek a model of the form:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-belly",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3  Load and Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-navigation",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with session_engine.connect() as conn:\n",
    "    petrol_consumption_df = pd.read_sql_query(text('SELECT * FROM petrol_consumption'), conn)\n",
    "\n",
    "petrol_consumption_df.drop('index', axis=1, inplace=True)\n",
    "petrol_consumption_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-third",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "petrol_consumption_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-command",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.4 Preprocess the Data & split to train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-bones",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = petrol_consumption_df.drop('Petrol_Consumption', axis=1)\n",
    "y = petrol_consumption_df['Petrol_Consumption']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-rebound",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.5 Build, Train and Test Model Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-profit",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "print(f\"Model intercept : {regressor.intercept_}\")\n",
    "print(f\"Model coefficients : {regressor.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-catch",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.6 Evaluate model using appropiate evaluation metrics\n",
    "\n",
    "<span style=\"color:red\">Task 1 : Use the predict function in the model(`regressor`) to predict the output of the testset(`X_test`) and  print it. </span>\n",
    "\n",
    "<span style=\"color:red\">Task  2 : Measure the performance of the model using: <br>     </span>\n",
    "1. root mean squared error \n",
    "1. mean absolute error \n",
    "1. coefficient of determination $R^2$. See [`sklearn.metrics.r2_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) for $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-cornell",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-crowd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-population",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Polynomial Regression, Underfitting & Overfitting\n",
    "\n",
    "Polynomial Regression, is simply a transformation for the explanatory variables to higher polynomial orders with interactive variables.\n",
    "\n",
    "1. Why do we need it?\n",
    "1. Which order of the polynomial should we choose? \n",
    "2. should we always pick the most complex model? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-circulation",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-bradley",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures #to convert the original features into their higher order terms \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(PolynomialFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-currency",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Load and Explore the dataset\n",
    "<span style=\"color:red\">Task : generate synthetic dataset by adding some random gaussian noise to a cosine function.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-barrier",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  function\n",
    "def fn(X):\n",
    "    #Task : one line of code\n",
    "    return None\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = fn(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.scatter(X,y,label=\"Samples\")\n",
    "plt.title('synthetic dataset')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-monday",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.4 Preprocess the Data, build model and evaluate model \n",
    "\n",
    "* We will build three polynomial models with degrees [1, 4, 15] and observe the effect of increasing the degree of complixity of the model on how well it suits the data.\n",
    "* We will evaluate our model using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-obligation",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "degrees = [1, 4, 15]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i])\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         \n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, fn(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-cameroon",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <center>Self practice</center>\n",
    "\n",
    "* Using steel industry energy consumption dataset from ([Link](https://archive.ics.uci.edu/dataset/851/steel+industry+energy+consumption)) or `ml-regression.db`.\n",
    "    1. Split the data to train and test set (80% training). Use [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "    1. Train a multiple linear regression model that will predict the energy consumption `(Usage_kWh)`.\n",
    "    1. Print the model `Mean Absolute Error`, `Mean Squared Error` and `Root Mean Squared Error`\n",
    "\n",
    "\n",
    "* Select one indipendent and select `Usage_kWh` as dependent variable \n",
    "    1. Visualise the data (using `matplotlib`)\n",
    "    1. Create a Polynomial Regression model using `[1,5,24]` as degrees\n",
    "    1. Explain your observations in terms of Over-fitting, Under-fitting, Bias-Variance trade-offs.\n",
    "\n",
    "* Write a brief explanation of the insights that you got from the datasets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab79f6-3683-40c8-80c1-7b07a6aa8bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
